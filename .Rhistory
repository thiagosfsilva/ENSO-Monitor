7,9,1),
ncol=3,
byrow=TRUE)
proc_raster <- function(inras){
sv <- rast(inras)
# plot(sv,col=class_col[1:4])
dw_crop <- resample(DW,y=sv,method='near')
#plot(dw_crop)
dw_crop[is.na(dw_crop)] <- 9
dw_rec <- classify(dw_crop,rec_mat)
# plot(dw_rec,col=class_col[1:6])
rep_ind <-  which(!is.na(values(dw_rec)))
values(sv)[rep_ind] <- values(dw_rec)[rep_ind]
#plot(sv,col=class_col[1:6])
fname <- basename(file_path_sans_ext(inras))
fpath <- paste0("C:/Projects/NGS_Amazon_mapping/output/sieve_comb/",fname,'_comb.tif')
writeRaster(sv,fpath,filetype='GTiff',datatype='INT1U')
}
lapply(sieved,proc_raster)
# Plot preci and water level from ANA
library(dplyr)
library(ggplot2)
library(tidyr)
library(lubridate)
library(scales)
library(httr)
######  ANA GET requst
ana_url <- 'http://telemetriaws1.ana.gov.br/ServiceANA.asmx/DadosHidrometeorologicos'
cod_esta <- '12850000'
data_ini <- '1900-01-01'
data_fim <- '2032-12-01'
r <- GET(ana_url, accept_json(), query = list(codEstacao = cod_esta,
dataInicio = data_ini,
dataFim= data_fim))
# rawToChar() will convert raw data
# to char and store in response variable
response < - rawToChar(r$content)
# rawToChar() will convert raw data
# to char and store in response variable
response <- rawToChar(r$content)
# print response
print(response)
library(JSON)
install.packages("JSON")
install.packages("tidyve")
install.packages(c("tidyverse", "jsonlite"))
library(tidyverse)
ana_url <- 'http://telemetriaws1.ana.gov.br/ServiceANA.asmx/DadosHidrometeorologicos'
cod_esta <- '12850000'
data_ini <- '1900-01-01'
data_fim <- '2032-12-01'
r <- GET(ana_url, accept_json(), query = list(codEstacao = cod_esta,
dataInicio = data_ini,
dataFim= data_fim))
library(httr)
?GET
response <- GET(ana_url, query = list(codEstacao = cod_esta,
dataInicio = data_ini,
dataFim= data_fim)) %>%
content()
# get names of the categories
category_names <- pluck(raw_json, "categories", 1, "labels") %>% tolower()
# get names of the categories
category_names <- pluck(response, "categories", 1, "labels") %>% tolower()
# Exploring the data levels
str(response,max.level = 1)
# Exploring the data levels
str(response$doc,max.level = 1)
# Exploring the data levels
str(response$node,max.level = 1)
install.packages("xml2")
library(xml2)
# Parse xml data
cotas_xml = as_list(read_xml(response))
# Parse xml data
cotas_xml = as_list(read_xml(response$doc))
?xml2
?read_xml
str(content)
str(response)
response <- response %>% rawToChar()
response <- rawToChar(response)
# Get the data
response <- GET(ana_url, query = list(codEstacao = cod_esta,
dataInicio = data_ini,
dataFim= data_fim)) %>%
rawToChar()
x <- c(1,2,3,4,5,6)
y <- c(5,6,7,8,9)
match(x,y)
intersect(x,y)
x %in% y
y[x %in% y]
x[x %in% y]
?mclaplly
parallel::
?parallel::mclapply
raster
library(raster)
mwtd <- raster('C:/Users/Thiago/Downloads/SAMERICA_WTD_annualmean.nc')
install.packages("ncdf4")
mwtd <- raster('C:/Users/Thiago/Downloads/SAMERICA_WTD_annualmean.nc')
mwtd
plot(mwtd)
mwtd <- raster('C:/Users/Thiago/Downloads/SAMERICA_WTD_monthlymeans.nc')
mwtd
library(sf)
library(terra)
library(sf)
awtd <- rast('C:/Users/Thiago/Downloads/SAMERICA_WTD_annualmean.nc')
mwtd <- rast('C:/Users/Thiago/Downloads/SAMERICA_WTD_monthlymeans.nc')
plots <- st_read('C:/Users/Thiago/Downloads/cwm50_input_map_mcwd_29sep23.gpkg')
mwtd
p_awtd <- extract(awtd,plots)
p_awtd
p_mwtd <- extract(mwtd,plots)
p_mwtd
wtd_df <- cbind(plots,p_awtd,p_mwtd)
libreray(dplyr)
library(dplyr)
?select
wrd_df <- wtd_df %>% select(-c(ID,mask,ID.1,mask.1)
)
View(wrd_df)
write.csv(wtd_df,'cwm50_input_map_mcwd+wtd_06oct23.csv',row.names = FALSE)
getwd()
source("C:/Users/Thiago/Documents/wtd_extract.R", echo=TRUE)
source("C:/Users/Thiago/OneDrive - University of Stirling/Manuscripts/jurua_blurb/analysis/analysis.R", echo=TRUE)
library(dplyr)
library(ggplot2)
library(tidyr)
library(lubridate)
library(scales)
library(httr)
library(here)
library(reticulate)
prec1 <- read.csv('chuvas_C_00266000_daily.csv')
getwd()
source("C:/Users/Thiago/OneDrive - University of Stirling/Manuscripts/jurua_blurb/analysis/analysis.R", echo=TRUE)
library(dplyr)
library(ggplot2)
library(reticulate)
pd <- import("pandas")
setwd('C:/Projects/ENSO-Monitor/')
df <- pd$read_pickle("data/hisCota.pkl")$reset_index()
pd <- import("pandas")
df <- pd$read_pickle("data/hisCota.pkl")$reset_index()
df <- pd$read_pickle("data/hisCota.pkl")
View(df)
p_load('dplyr','readr','here')
library(pacman)
p_load('dplyr','readr','here')
in_files <- here('data','cotas_C_12351000.csv')
?read_delim
in_data_raw <- read.delim(in_files, skip = 12, sep = ';', dec = ',', header = T)
View(in_data_raw)
p_load('dplyr','here','lubridate')
in_data <- in_data_raw %>% mutate(Data = parse_date_time(Data,orders='dd/mm/YY'))
in_data <- in_data_raw %>% mutate(Data = parse_date_time(Data,orders='d/m/Y'))
in_data <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='d/m/Y')) %>%
filter(MediaDiaria==1 & NivelConsistencia=1)
in_data <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='d/m/Y')) %>%
filter(MediaDiaria==1 & NivelConsistencia==1)
View(in_data)
View(in_data_raw)
in_data <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='d/m/Y')) %>%
filter(MediaDiaria==1)
in_data <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='d/m/Y')) %>%
filter(MediaDiaria==1) %>%
select(EstacaoCodigo,Data,starts_with('Cota'))
?select
in_data <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='d/m/Y')) %>%
filter(MediaDiaria==1) %>%
select(EstacaoCodigo,Data,starts_with('Cota')) %>%
select(!ends_with('Status'))
p_load('dplyr','here','lubridate','tidyr')
# Stack data
in_long <- pivot_longer(cols=!(EstacaoCodigo,Data),names_to='Dia',values_to='Nivel')
# Stack data
in_long <- pivot_longer(cols=!c('EstacaoCodigo','Data'),names_to='Dia',values_to='Nivel')
# Stack data
in_long <- pivot_longer(cols=!EstacaoCodigo,Data,names_to='Dia',values_to='Nivel')
?pivot_longer
# Stack data
in_long <- pivot_longer(cols=!starts_with(Cota),Data,names_to='Dia',values_to='Nivel')
# Stack data
in_long <- pivot_longer(cols=!starts_with(Cota),names_to='Dia',values_to='Nivel')
# Stack data
in_long <- pivot_longer(cols=!starts_with('Cota'),names_to='Dia',values_to='Nivel')
# Stack data
in_long <- pivot_longer(cols = !starts_with('Cota'),
names_to='Dia',
values_to='Nivel')
# Stack data
in_long <- ind_data %>% pivot_longer(cols = !starts_with('Cota'),
names_to='Dia',
values_to='Nivel')
# Stack data
in_long <- in_data %>% pivot_longer(cols = !starts_with('Cota'),
names_to='Dia',
values_to='Nivel')
# Stack data
in_long <- in_data %>% pivot_longer(cols = Data),
# Stack data
in_long <- in_data %>% pivot_longer(cols = Data,
names_to='Dia',
values_to='Nivel')
# Stack data
in_long <- in_data %>% pivot_longer(cols = starts_with('Cota'),
names_to='Dia',
values_to='Nivel')
View(in_long)
# Stack data and split date components
in_long <- in_data %>%
pivot_longer(cols = starts_with('Cota'),
names_to='Dia',
values_to='Nivel') %>%
mutate(yr = year(Data), mn=month(Data))
p_load('dplyr','here','lubridate','tidyr','stringr')
str_extract('Cota01',"[0-9]")
str_extract('Cota01',"[0-9]*")
str_extract('Cota01',"*[0-9]")
str_extract('Cota01',"\d+")
str_extract('Cota01',"\\d+")
str_extract('Cota31',"\\d+")
# Stack data and split date components
in_long <- in_data %>%
pivot_longer(cols = starts_with('Cota'),
names_to='Dia',
values_to='Nivel') %>%
mutate(yr = year(Data), mn=month(Data), dy = as.numeric(str_extract(Dia,"\\d+")))
p_load('dplyr','here','lubridate','tidyr','stringr','glue')
# Stack data and split date components
in_long <- in_data %>%
pivot_longer(cols = starts_with('Cota'),
names_to='Dia',
values_to='Nivel') %>%
mutate(yr = year(Data),
mn=month(Data),
dy = as.numeric(str_extract(Dia,"\\d+"))) %>%
mutate(dt = as.Date(glue('{yr}-{mn}-{dy}')))
# Clean up data
in_long <- in_data %>%
pivot_longer(cols = starts_with('Cota'),
names_to='Dia',
values_to='Nivel') %>% # stacks dates
mutate(yr = year(Data),
mn=month(Data),
dy = as.numeric(str_extract(Dia,"\\d+"))) %>% # gets Y, m, d as ints
mutate(dt = as.Date(glue('{yr}-{mn}-{dy}'))) %>%  # Build date
filter(!is.na(dt)) # Remove impossible dates
# Check data consistency per year
yr_obs <- in_long %>% group_by(yr) %>%
mutate(nobs = count(Nivel))
# Check data consistency per year
yr_obs <- in_long %>% group_by(yr) %>%
mutate(nobs = count())
?count
# Check data consistency per year
yr_obs <- in_long %>% group_by(yr) %>%
add_count(nobs)
# Check data consistency per year
yr_obs <- in_long %>% group_by(yr) %>%
add_count()
View(yr_obs)
# Check data consistency per year
yr_obs <- in_long %>% group_by(yr) %>%
summarise(nobs = count(Nivel))
# Check data consistency per year
yr_obs <- in_long %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
View(yr_obs)
# Clean up data
in_long <- in_data %>%
pivot_longer(cols = starts_with('Cota'),
names_to='Dia',
values_to='Nivel') %>% # stacks dates
mutate(yr = year(Data),
mn=month(Data),
dy = as.numeric(str_extract(Dia,"\\d+"))) %>% # gets Y, m, d as ints
mutate(dt = as.Date(glue('{yr}-{mn}-{dy}'))) %>%  # Build date
filter(!is.na(dt)) %>% # Remove impossible dates
filter(!(mn==2 & dy == 29)) # Remove stupid leap days
# Check data consistency per year
yr_obs <- in_long %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
df1987 <- in_long %>% filter(yr=1987)
df1987 <- in_long %>% filter(yr==1987)
View(df1987)
df2017 <- in_long %>% filter(yr=2017)
df2017 <- in_long %>% filter(yr==2017)
View(df2017)
View(in_data_raw)
df2017 %>% filter(dy == 1)
print(n=30)
print(df2017 %>% filter(dy == 1),n=30)
in_data_raw %>% group_by(NivelConsistencia,MediaDiaria) %>% summarise(length(Media))
View(in_data)
in_data <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='d/m/Y')) %>%
filter(MediaDiaria==1) %>%
#select(EstacaoCodigo,Data,starts_with('Cota')) %>%
select(!ends_with('Status'))
in_data <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='d/m/Y')) %>%
filter(MediaDiaria==1 & NivelConsistencia==1) %>%
select(EstacaoCodigo,Data,starts_with('Cota')) %>%
select(!ends_with('Status'))
# Clean up data
in_long <- in_data %>%
pivot_longer(cols = starts_with('Cota'),
names_to='Dia',
values_to='Nivel') %>% # stacks dates
mutate(yr = year(Data),
mn=month(Data),
dy = as.numeric(str_extract(Dia,"\\d+"))) %>% # gets Y, m, d as ints
mutate(dt = as.Date(glue('{yr}-{mn}-{dy}'))) %>%  # Build date
filter(!is.na(dt)) %>% # Remove impossible dates
filter(!(mn==2 & dy == 29)) %>%  # Remove stupid leap days
# Check data consistency per year
yr_obs <- in_long %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
# Clean up data
in_long <- in_data %>%
pivot_longer(cols = starts_with('Cota'),
names_to='Dia',
values_to='Nivel') %>% # stacks dates
mutate(yr = year(Data),
mn=month(Data),
dy = as.numeric(str_extract(Dia,"\\d+"))) %>% # gets Y, m, d as ints
mutate(dt = as.Date(glue('{yr}-{mn}-{dy}'))) %>%  # Build date
filter(!is.na(dt)) %>% # Remove impossible dates
filter(!(mn==2 & dy == 29)) %>%  # Remove stupid leap days
# Check data consistency per year
yr_obs <- in_long %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
in_data <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='d/m/Y')) %>%
filter(MediaDiaria==1 & NivelConsistencia==1) %>%
select(EstacaoCodigo,Data,starts_with('Cota')) %>%
select(!ends_with('Status'))
# Clean up data
in_long <- in_data %>%
pivot_longer(cols = starts_with('Cota'),
names_to='Dia',
values_to='Nivel') %>% # stacks dates
mutate(yr = year(Data),
mn=month(Data),
dy = as.numeric(str_extract(Dia,"\\d+"))) %>% # gets Y, m, d as ints
mutate(dt = as.Date(glue('{yr}-{mn}-{dy}'))) %>%  # Build date
filter(!is.na(dt)) %>% # Remove impossible dates
filter(!(mn==2 & dy == 29))  # Remove stupid leap days
# Check data consistency per year
yr_obs <- in_long %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
in_data_nc2 <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='d/m/Y')) %>%
filter(MediaDiaria==1 & NivelConsistencia==2) %>%
select(EstacaoCodigo,Data,starts_with('Cota')) %>%
select(!ends_with('Status'))
View(in_data_nc2)
in_data <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='d/m/Y')) %>%
filter(MediaDiaria==1 & NivelConsistencia==2) %>%
select(EstacaoCodigo,Data,starts_with('Cota')) %>%
select(!ends_with('Status'))
# Clean up data
in_long <- in_data %>%
pivot_longer(cols = starts_with('Cota'),
names_to='Dia',
values_to='Nivel') %>% # stacks dates
mutate(yr = year(Data),
mn=month(Data),
dy = as.numeric(str_extract(Dia,"\\d+"))) %>% # gets Y, m, d as ints
mutate(dt = as.Date(glue('{yr}-{mn}-{dy}'))) %>%  # Build date
filter(!is.na(dt)) %>% # Remove impossible dates
filter(!(mn==2 & dy == 29))  # Remove stupid leap days
# Check data consistency per year
yr_obs <- in_long %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
full_yrs <- yr_obs %>% filter(nobs=365) %>% select(yr)
full_yrs <- yr_obs %>% filter(nobs==365) %>% select(yr)
?filter
# Remove years without 365 obs
in_clean <- in_long %>% filter(yr %in% full_yrs)
yr_clan_obs <- in_clean %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
View(yr_clan_obs)
View(in_clean)
full_yrs
1998 %in% full_yrs
1998 %in% full_yrs$yr
# Remove years without 365 obs
in_clean <- in_long %>% filter(yr %in% full_yrs$yr)
yr_clean_obs <- in_clean %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
View(yr_clean_obs)
p_load('dplyr','here','lubridate','tidyr','stringr','glue','arrow')
# Remove years without 365 obs
in_clean <- in_long %>% filter(yr %in% full_yrs$yr) %>%
select(!(Data,Dia))
# Remove years without 365 obs
in_clean <- in_long %>% filter(yr %in% full_yrs$yr) %>%
select(!one_of('Data','Dia'))
# Remove years without 365 obs
in_clean <- in_long %>% filter(yr %in% full_yrs$yr) %>%
select(!one_of('Data','Dia')) %>%
rename(id=EstacaoCodigo,level=Nivel)
here()
# Save result as parquet file
write_parquet(in_clean,here('data','historicalLevels.parquet'))
station <- in_clean$id[1]
# Save result as parquet file
write_parquet(in_clean,here('data',glue('historicalLevels_{station}.parquet'))
)
source("C:/Projects/ENSO-Monitor/clean_ana_cotas.R", echo=TRUE)
library(pacman)
p_load('dplyr','here','lubridate','tidyr','stringr','glue','arrow')
# Set input file(s)
in_files <- here('data','cotas_C_12351000.csv')
# Load and format raw data
in_data_raw <- read.delim(in_files, skip = 12, sep = ';', dec = ',', header = T)
in_data <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='d/m/Y')) %>%
filter(MediaDiaria==1 & NivelConsistencia==2) %>%
select(EstacaoCodigo,Data,starts_with('Cota')) %>%
select(!ends_with('Status'))
# Clean up data
in_long <- in_data %>%
pivot_longer(cols = starts_with('Cota'),
names_to='Dia',
values_to='Nivel') %>% # stacks dates
mutate(yr = year(Data),
mn=month(Data),
dy = as.numeric(str_extract(Dia,"\\d+"))) %>% # gets Y, m, d as ints
mutate(dt = as.Date(glue('{yr}-{mn}-{dy}'))) %>%  # Build date
mutate(doy = yday(dt)) %>% # Add day of year column
filter(!is.na(dt)) %>% # Remove impossible dates
filter(!(mn==2 & dy == 29))  # Remove stupid leap days
# Check data consistency per year
yr_obs <- in_long %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
full_yrs <- yr_obs %>% filter(nobs==365) %>% select(yr)
# Remove years without 365 obs
in_clean <- in_long %>% filter(yr %in% full_yrs$yr) %>%
select(!one_of('Data','Dia')) %>%
rename(id=EstacaoCodigo, level=Nivel)
yr_clean_obs <- in_clean %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
yr_clean_obs <- in_clean %>% group_by(yr) %>%
summarise(nobs = length(level))
station <- in_clean$id[1]
# Save result as parquet file
write_parquet(in_clean,here('data',glue('historicalLevels_{station}.parquet')))
old <- read_parquet('currentData_prehack.parquet')
View(old)
newdf <- old %>% select(-Vazao))  %>%
newdf <- old %>% select(-Vazao)  %>%
rename(level=Nivel,precip=Chuva,dt=Dt,doy=Doy)
newdf <- old %>% select(-Vazao)  %>%
rename(level=Nivel,precip=Chuva,dt=Dt,doy=Doy) %>%
relocate(dt,doy,level,precip)
write_parquet(newdf,'data/currentData_prehack.parquet')
old %>% mutate(mn = month(dt)) %>% group_by(mn) %>% summarise(count=length(Nivel))
old %>% mutate(mn = month(Dt)) %>% group_by(mn) %>% summarise(count=length(Nivel))
newdf %>% mutate(mn = month(dt)) %>% group_by(mn) %>% summarise(count=length(Nivel))
newdf %>% mutate(mn = month(dt)) %>% group_by(mn) %>% summarise(count=length(level))
ndwdf2 <- na.omit(newdf)
View(newdf)
View(ndwdf2)
write_parquet(ndwdf2,'data/currentData_prehack.parquet')
# Set input file(s)
in_files <- here('data','cotas_C_12351000.csv')
# Load and format raw data
in_data_raw <- read.delim(in_files, skip = 12, sep = ';', dec = ',', header = T)
in_data <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='d/m/Y')) %>%
filter(MediaDiaria==1 & NivelConsistencia==2) %>%
select(EstacaoCodigo,Data,starts_with('Cota')) %>%
select(!ends_with('Status'))
# Clean up data
in_long <- in_data %>%
pivot_longer(cols = starts_with('Cota'),
names_to='Dia',
values_to='Nivel') %>% # stacks dates
mutate(yr = year(Data),
mn=month(Data),
dy = as.numeric(str_extract(Dia,"\\d+"))) %>% # gets Y, m, d as ints
mutate(dt = as.Date(glue('{yr}-{mn}-{dy}'))) %>%  # Build date
mutate(doy = yday(dt)) %>% # Add day of year column
filter(!is.na(dt)) %>% # Remove impossible dates
filter(!(mn==2 & dy == 29))  # Remove stupid leap days
# Check data consistency per year
yr_obs <- in_long %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
full_yrs <- yr_obs %>% filter(nobs==365) %>% select(yr)
# Remove years without 365 obs
in_clean <- in_long %>% filter(yr %in% full_yrs$yr) %>%
select('dt','doy','Nivel') %>%
rename(level=Nivel)
yr_clean_obs <- in_clean %>% group_by(yr) %>%
summarise(nobs = length(level))
station <- in_long$id[1]
station <- in_long$EstacaoCodigo[1]
# Save result as parquet file
write_parquet(in_clean,here('data',glue('historicalLevels_{station}.parquet')))
source("C:/Projects/ENSO-Monitor/clean_ana_cotas.R", echo=TRUE)
