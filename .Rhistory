# Save result as parquet file
write_parquet(in_clean,here('data',glue('historicalLevels_{station}.parquet')))
library(ggplot)
library(ggplot2)
ggplot(in_clean,aes(doy,level)) + geom_line(aes(color=yr))
ggplot(in_clean,aes(doy,level)) + geom_line(aes(color=yr, group=yr))
ggplot(filter(in_clean, yr=2015),aes(doy,level)) + geom_line(aes(color=yr, group=yr))
ggplot(filter(in_clean, yr==2015),aes(doy,level)) + geom_line(aes(color=yr, group=yr))
ggplot(filter(in_clean, yr==1998),aes(doy,level)) + geom_line(aes(color=yr, group=yr))
ggplot(filter(in_clean, yr==2022),aes(doy,level)) + geom_line(aes(color=yr, group=yr))
ggplot(filter(in_clean, yr==2000),aes(doy,level)) + geom_line(aes(color=yr, group=yr))
ggplot(filter(in_clean, yr==2005),aes(doy,level)) + geom_line(aes(color=yr, group=yr))
ggplot(filter(in_clean, yr==2004),aes(doy,level)) + geom_line(aes(color=yr, group=yr))
ggplot(in_clean,aes(doy,level)) + geom_line(aes(color=yr, group=yr))
station
# Check data consistency yet again
yr_obs <- in_clean %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
# Check data consistency yet again
yr_obs <- in_clean %>% group_by(yr) %>%
summarise(nobs = length(level))
View(yr_obs)
# Set input file(s)
in_files <- here('data','cotas_C_12351000.csv')
# Load and format raw data
in_data_raw <- read.delim(in_files, skip = 12, sep = ';', dec = ',', header = T)
in_data <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='d/m/Y')) %>%
filter(MediaDiaria==1) %>%
select(EstacaoCodigo,Data,starts_with('Cota')) %>%
select(!ends_with('Status'))
# Clean up data
in_long <- in_data %>%
pivot_longer(cols = starts_with('Cota'),
names_to='Dia',
values_to='Nivel') %>% # stacks dates
mutate(yr = year(Data),
mn=month(Data),
dy = as.numeric(str_extract(Dia,"\\d+"))) %>% # gets Y, m, d as ints
mutate(dt = as.Date(glue('{yr}-{mn}-{dy}'))) %>%  # Build date
mutate(doy = yday(dt)) %>% # Add day of year column
filter(!is.na(dt)) %>% # Remove impossible dates
filter(!(mn==2 & dy == 29))  # Remove stupid leap days
# Check data consistency per year
yr_obs <- in_long %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
print(yr_obs)
print(yr_obs, n=47)
# Remove duplicates based on date
in_cons <- in_long %>% distinct(dt, .keep_all = TRUE)
# Remove duplicates based on date
in_cons <- in_long %>% distinct(dt, .keep_all = TRUE)
# Check data consistency again
yr_obs <- in_cons %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
print(yr_obs, n=47)
full_yrs <- yr_obs %>% filter(nobs==365) %>% select(yr)
# Remove years without 365 obs
in_clean <- in_cons %>% filter(yr %in% full_yrs$yr) %>%
select('dt','doy','yr','Nivel') %>%
arrange(dt) %>%
rename(level=Nivel)
# Check data consistency yet again
yr_obs <- in_clean %>% group_by(yr) %>%
summarise(nobs = length(level))
print(yr_obs, n=47)
station <- in_long$EstacaoCodigo[1]
# Save result as parquet file
write_parquet(in_clean,here('data',glue('historicalLevels_{station}.parquet')))
summary(in_clean)
in_clean %>% filter(doy == 366)
# Days of the year get messed up because they will still show 366, so we need to
# rescale all leap years to 1 - 365
nyears <- nrow(yr_obs)
ydays <- repeat(seq(1,365),nyears)
ydays <- repeat(seq(1,365), nyears)
?repeat
repeat
rep
ydays <- rep(seq(1,365), nyears)
in_clean$doy <- ydays
# Save result as parquet file
write_parquet(in_clean,here('data',glue('historicalLevels_{station}.parquet')))
ggplot(in_clean,aes(doy,level)) + geom_line(aes(color=yr, group=yr))
?range
library(pacman)
p_load('dplyr','here','lubridate','tidyr','stringr','glue','arrow')
in_files <- list.files(here('data','conventional'),
pattern='csv$',
full.names = TRUE)
in_files
range(in_files)
range(length(in_files))
st=1
# Load and format raw data
in_data_raw <- read.delim(in_files[st],
skip = 12,
sep = ';',
dec = ',',
header = T)
in_files[st]
# Load and format raw data
in_data_raw <- read.delim(in_files[st],
#skip = 12,
sep = ';',
dec = '.',
header = T)
View(in_data_raw)
in_data <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='m/Y')) %>%
filter(MediaDiaria==1 & NivelConsistencia==2) %>%
select(EstacaoCodigo,Data,starts_with('Cota')) %>%
select(!ends_with('Status'))
in_data <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='m/Y')) %>%
filter(MediaDiaria==1) %>%
select(EstacaoCodigo,Data,starts_with('Cota')) %>%
select(!ends_with('Status'))
head(in_data)
tail(in_data)
# Clean up data
in_long <- in_data %>%
pivot_longer(cols = starts_with('Cota'),
names_to='Dia',
values_to='Nivel') %>% # stacks dates
mutate(yr = year(Data),
mn=month(Data),
dy = as.numeric(str_extract(Dia,"\\d+"))) %>% # gets Y, m, d as ints
mutate(dt = as.Date(glue('{yr}-{mn}-{dy}'))) %>%  # Build date
mutate(doy = yday(dt)) %>% # Add day of year column
filter(!is.na(dt)) %>% # Remove impossible dates
filter(!(mn==2 & dy == 29))  # Remove stupid leap days
View(in_long)
# Check data consistency per year
yr_obs <- in_long %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
print(yr_obs, n=50)
ind <- duplicated(in_long$dt)
ind
dupes <- in_long[ind,]
head(dupes)
in_data <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='m/Y')) %>%
filter(MediaDiaria==1) %>%
select(EstacaoCodigo,Data,NivelConsistencia,starts_with('Cota')) %>%
select(!ends_with('Status'))
# Clean up data
in_long <- in_data %>%
pivot_longer(cols = starts_with('Cota'),
names_to='Dia',
values_to='Nivel') %>% # stacks dates
mutate(yr = year(Data),
mn=month(Data),
dy = as.numeric(str_extract(Dia,"\\d+"))) %>% # gets Y, m, d as ints
mutate(dt = as.Date(glue('{yr}-{mn}-{dy}'))) %>%  # Build date
mutate(doy = yday(dt)) %>% # Add day of year column
filter(!is.na(dt)) %>% # Remove impossible dates
filter(!(mn==2 & dy == 29))  # Remove stupid leap days
summary(in_long)
ind <- duplicated(in_long$dt)
dupes <- in_long[ind,]
head(dupes)
head(in_long)
?duplicated
ind <- duplicated(in_long$dt, fromLast = TRUE)
dupes <- in_long[ind,]
head(dupes)
ind <- duplicated(in_long$dt)
dupes <- in_long[ind,]
head(dupes)
in_dedup <- in_long[!ind]
in_dedup <- in_long[!ind,]
View(in_dedup)
# Check data consistency yet again
yr_obs <- in_dedup %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
print(yr_obs, n=50)
# Get only years with complete observations
full_yrs <- yr_obs %>% filter(nobs==365) %>% select(yr)
# Remove years without 365 obs
in_clean <- in_long %>% filter(yr %in% full_yrs$yr) %>%
select('dt','doy','yr','Nivel') %>%
rename(level=Nivel)
# Check data consistency a third time
yr_obs <- in_clean %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
print(yr_obs, n=50)
full_yrs
print(full_yrs, n=50)
# Remove years without 365 obs
in_clean <- in_long %>% filter(yr %in% full_yrs$yr) %>%
select('dt','doy','yr','Nivel') %>%
rename(level=Nivel)
# Check data consistency a third time
yr_obs <- in_clean %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
# Check data consistency a third time
yr_obs <- in_clean %>% group_by(yr) %>%
summarise(nobs = length(level))
print(yr_obs, n=50)
# Remove years without 365 obs
in_clean <- in_dedup %>% filter(yr %in% full_yrs$yr) %>%
select('dt','doy','yr','Nivel') %>%
rename(level=Nivel)
# Check data consistency a third time
yr_obs <- in_clean %>% group_by(yr) %>%
summarise(nobs = length(level))
print(yr_obs, n=50)
# We need 'fake' doys since we remove Feb 29th, but the calculated doys will
# Still go to 366 if calculated using yday()
nyears <- nrow(yr_obs)
ydays <- rep(seq(1,365),nyears)
in_clean$doy <- ydays
station <- in_long$EstacaoCodigo[1]
# Save result as parquet file
write_parquet(in_clean,here('data',glue('historicalLevels_{station}.parquet')))
### Test plot
library(ggplot2)
ggplot(in_clean,aes(doy,level)) + geom_line(aes(group=yr))
?title
ggplot(in_clean,aes(doy,level)) +
geom_line(aes(group=yr)) +
labs(title=station)
in_files <- list.files(here('data','conventional'),
pattern='csv$',
full.names = TRUE)
st=1
# Load and format raw data
in_data_raw <- read.delim(in_files[st],
#skip = 12,
sep = ';',
dec = '.',
header = T)
in_data <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='m/Y')) %>%
filter(MediaDiaria==1) %>%
select(EstacaoCodigo,Data,NivelConsistencia,starts_with('Cota')) %>%
select(!ends_with('Status'))
# Clean up data
in_long <- in_data %>%
pivot_longer(cols = starts_with('Cota'),
names_to='Dia',
values_to='Nivel') %>% # stacks dates
mutate(yr = year(Data),
mn=month(Data),
dy = as.numeric(str_extract(Dia,"\\d+"))) %>% # gets Y, m, d as ints
mutate(dt = as.Date(glue('{yr}-{mn}-{dy}'))) %>%  # Build date
mutate(doy = yday(dt)) %>% # Add day of year column
filter(!is.na(dt)) %>% # Remove impossible dates
filter(!(mn==2 & dy == 29))  # Remove stupid leap days
# Check data consistency per year
yr_obs <- in_long %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
# Deduplicate by taking only NivelCinsistecia if both exist
ind <- duplicated(in_long$dt)
in_dedup <- in_long[!ind,]
# Check data consistency yet again
yr_obs <- in_dedup %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
# Get only years with complete observations
full_yrs <- yr_obs %>% filter(nobs==365) %>% select(yr)
# Remove years without 365 obs
in_clean <- in_dedup %>% filter(yr %in% full_yrs$yr) %>%
select('EstacaoCodigo','dt','doy','yr','Nivel') %>%
rename(level=Nivel,station=EstacaoCodigo)
# Check data consistency a third time
yr_obs <- in_clean %>% group_by(yr) %>%
summarise(nobs = length(level))
print(yr_obs, n=50)
# We need 'fake' doys since we remove Feb 29th, but the calculated doys will
# Still go to 366 if calculated using yday()
nyears <- nrow(yr_obs)
ydays <- rep(seq(1,365),nyears)
in_clean$doy <- ydays
head(in_clean)
# Create empy df to hold all stations
out_data <- data.frame(station=numeric(), dt=Date(), doy = numeric(),
yr = numeric(), level = numeric())
out_data <- rbind(out_data,in_clean)
names(in_data_raw)
?break
# TEST 1: are the first and last column names correct?
if (names(in_data_raw)[1]=='EstacaoCodigo' &
names(in_data_raw)[ncol(in_data_raw)]=='Cota31Status'){
print('Data reading test passed')
} else {
print('Data reading test failed')
break
}
in_data <- in_data_raw %>%
mutate(Data = parse_date_time(Data,orders='m/Y')) %>%
filter(MediaDiaria==1) %>%
select(EstacaoCodigo,Data,NivelConsistencia,starts_with('Cota')) %>%
select(!ends_with('Status'))
# Clean up data
in_long <- in_data %>%
pivot_longer(cols = starts_with('Cota'),
names_to='Dia',
values_to='Nivel') %>% # stacks dates
mutate(yr = year(Data),
mn=month(Data),
dy = as.numeric(str_extract(Dia,"\\d+"))) %>% # gets Y, m, d as ints
mutate(dt = as.Date(glue('{yr}-{mn}-{dy}'))) %>%  # Build date
mutate(doy = yday(dt)) %>% # Add day of year column
filter(!is.na(dt)) %>% # Remove impossible dates
filter(!(mn==2 & dy == 29))  # Remove stupid leap days
# Check data consistency per year
yr_obs <- in_long %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
print(yr_obs, n=50)
# Deduplicate by taking only NivelConsistencia == 2 if both exist
ind <- duplicated(in_long$dt)
in_dedup <- in_long[!ind,]
# Check data consistency yet again
yr_obs <- in_dedup %>% group_by(yr) %>%
summarise(nobs = length(Nivel))
print(yr_obs, n=50)
source("C:/Users/Thiago/Projects/ENSO-Monitor/clean_ana_cotas.R", echo=TRUE)
ggplot(in_clean,aes(doy,level)) +
geom_line(aes(group=yr)) +
facet_wrap(~station)
summary(out_data)
ggplot(in_clean,aes(doy,level)) +
geom_line(aes(group=yr)) +
facet_wrap(~factor(station))
out_data$station <- as.Factor(out_data$station)
out_data$station <- as.factor(out_data$station)
head(out_data)
ggplot(in_clean,aes(doy,level)) +
geom_line(aes(group=yr)) +
facet_wrap(~factor(station))
ggplot(in_clean,aes(doy,level)) +
geom_line(aes(group=yr)) +
facet_wrap(station)
ggplot(in_clean,aes(doy,level)) +
geom_line(aes(group=yr)) +
facet_wrap(vars(station))
ggplot(out_data,aes(doy,level)) +
geom_line(aes(group=yr)) +
facet_wrap(vars(station))
in_files
source("C:/Users/Thiago/Projects/ENSO-Monitor/clean_ana_cotas.R", echo=TRUE)
# Load packages
library(arrow)
library(here)
# Load historical water level data from Fonte Boa
fb_levels <- read_parquet(here('data/historicalLevels_12351000.parquet'))
library(ggplot2)
fb_2023 <- read_parquet(here('data/currentData.parquet'))
tail(fb_levels)
fb_all <-
# Plot all years
ggplot(fb_levels,aes(doy,level)) + geom_line(aes(group=yr))
fb_all <-
# Plot all years
ggplot(fb_levels,aes(doy,level)) + geom_line(aes(group=yr))
library(ggplot2)
# Plot all years
ggplot(fb_levels,aes(doy,level)) + geom_line(aes(group=yr))
library(lubridate)
# Add months to data
fb_levels$mn <- month(fb_levels$dt)
fb_2023$mn <- month(fb_2023$dt)
# Plot all years
ggplot(fb_levels,aes(doy,level)) + geom_line(aes(group=yr,color=mn))
?month
# Add months to data
fb_levels$mn <- as.factor(month(fb_levels$dt, label = TRUE))
fb_2023$mn <- as.factor(month(fb_2023$dt, label = TRUE))
# Plot all years
ggplot(fb_levels,aes(doy,level)) + geom_line(aes(group=yr,color=mn))
# Plot all years
ggplot(fb_levels,aes(doy,level)) +
geom_line(aes(group=yr,color=mn)) +
scale_color_discrete()
# NERC analysis of Amazon hydrology
# Load packages
library(arrow)
library(here)
library(ggplot2)
library(lubridate)
# Load historical water level data from Fonte Boa
fb_levels <- read_parquet(here('data/historicalLevels_12351000.parquet'))
fb_2023 <- read_parquet(here('data/currentData.parquet'))
# Add months to data
fb_levels$mn <- as.factor(month(fb_levels$dt, label = TRUE))
fb_2023$mn <- as.factor(month(fb_2023$dt, label = TRUE))
?arrange
library(dplyr)
lows <- fb_levels %>%
filter(doy >= 244 & doy <= 304) %>%
group_by(yr) %>%
arrange(level) %>%
slice(1:15) %>%
summarise(min_level = mean(level, na.rm=TRUE))
lows
highs <- fb_levels %>%
filter(doy >= 121 & doy <= 181) %>%
group_by(yr) %>%
arrange(level) %>%
slice(1:15) %>%
summarise(max_level = mean(level, na.rm=TRUE))
highs
min_ci <- c(mean(lows$min_level) - 1.96 * sd(lows$min_level),
)
min_ci <- c(mean(lows$min_level) - 1.96 * sd(lows$min_level),
mean(lows$min_level) + 1.96 * sd(lows$min_level))
min ci
min_ci
min_extremes <- lows$min_level <= min_ci[1] | lows$min_level >= min_ci[2]
min_extremes
avg_min <- mean(lows$min_level[1:35])
min_ci_35 <- c(mean(lows$min_level[1:35]) - 1.96 * sd(lows$min_level[1:35]),
mean(lows$min_level[1:35]) + 1.96 * sd(lows$min_level[1:35]))
min_extremes <- lows$min_level <= min_ci[1] | lows$min_level >= min_ci[2]
min_extremes
lows
min_ci_35
min_ci_35 <- c(mean(lows$min_level[1:35]) - 1 * sd(lows$min_level[1:35]),
mean(lows$min_level[1:35]) + 1 * sd(lows$min_level[1:35]))
min_extremes <- lows$min_level <= min_ci[1] | lows$min_level >= min_ci[2]
min_extremes
mon_ci
lows <- fb_levels %>%
filter(doy >= 244 & doy <= 304) %>%
group_by(yr) %>%
arrange(level) %>%
slice(1:15) %>%
summarise(min_level = mean(level, na.rm=TRUE)) %>%
arrange(min_level)
lows
low_2023 <- fb_2023 %>%
filter(doy >= 244 & doy <= 304) %>%
group_by(yr) %>%
arrange(level) %>%
slice(1:15) %>%
summarise(min_level = mean(level, na.rm=TRUE)) %>%
arrange(min_level)
low_2023 <- fb_2023 %>%
filter(doy >= 244 & doy <= 304) %>%
arrange(level) %>%
slice(1:15) %>%
summarise(min_level = mean(level, na.rm=TRUE)) %>%
arrange(min_level)
low_2023
high_2023 <- fb_2023 %>%
filter(doy >= 121 & doy <= 181) %>%
arrange(level) %>%
slice(1:15) %>%
summarise(min_level = mean(level, na.rm=TRUE))
lows[43,] <- c(2003,low_2023)
lows <- fb_levels %>%
filter(doy >= 244 & doy <= 304) %>%
group_by(yr) %>%
arrange(level) %>%
slice(1:15) %>%
summarise(min_level = mean(level, na.rm=TRUE)) %>%
arrange(min_level)
low_2023 <- fb_2023 %>%
filter(doy >= 244 & doy <= 304) %>%
arrange(level) %>%
slice(1:15) %>%
summarise(min_level = mean(level, na.rm=TRUE))
lows[43,] <- c(2023,low_2023)
highs <- fb_levels %>%
filter(doy >= 121 & doy <= 181) %>%
group_by(yr) %>%
arrange(level) %>%
slice(1:15) %>%
summarise(max_level = mean(level, na.rm=TRUE))
high_2023 <- fb_2023 %>%
filter(doy >= 121 & doy <= 181) %>%
arrange(level) %>%
slice(1:15) %>%
summarise(min_level = mean(level, na.rm=TRUE))
highs[43,] <- c(2023,high_2023)
lows %>% arrange(level)
lows %>% arrange(levels)
lows
lows %>% arrange(min_level)
print(lows %>% arrange(min_level), n=50)
print(highs %>% arrange(max_level), n=50)
print(highs %>% arrange(desc(max_level), n=50))
print(highs %>% arrange(desc(max_level)), n=50)
ac(lows$min_level)
acf(lows$min_level)
lows
highs <- fb_levels %>%
filter(doy >= 121 & doy <= 181) %>%
group_by(yr) %>%
arrange(level) %>%
slice(1:15) %>%
summarise(max_level = mean(level, na.rm=TRUE))
high_2023 <- fb_2023 %>%
filter(doy >= 121 & doy <= 181) %>%
arrange(level) %>%
slice(1:15) %>%
summarise(min_level = mean(level, na.rm=TRUE))
highs[43,] <- c(2023,high_2023)
lows
lows <- lows %>% arrange(yr)
highs <- highs %>% arrange(yr)
lows
acf(lows$min_level)
print(acf(lows$min_level))
print(acf(highs$max_level))
min(acf(lows$min_level),acf(highs$max_level))
min(acf(lows$min_level))
min(print((acf(lows$min_level)))
)
print(acf(lows$min_level))
print(acf(highs$max_level))
